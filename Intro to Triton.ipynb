{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb1be17f",
   "metadata": {},
   "source": [
    "## GPUs and challenges in programming them. Meet Triton\n",
    "\n",
    "<img src=\"https://openaicom.imgix.net/778bccdf-6cb5-4d9f-8811-f8df6da52d84/gpu-architecture.svg?fm=auto&auto=compress,format&fit=min&w=1919&h=612\" width=\"800\" height=\"500\">\n",
    "Source: https://openaicom.imgix.net/778bccdf-6cb5-4d9f-8811-f8df6da52d84/gpu-architecture.svg?fm=auto&auto=compress,format&fit=min&w=1919&h=612\n",
    "\n",
    "The architecture of modern GPUs can be roughly divided into three major components—DRAM, SRAM and ALUs—each of which must be considered when optimizing CUDA code:\n",
    "\n",
    "- Memory transfers from DRAM must be coalesced into large transactions to leverage the large bus width of modern memory interfaces.\n",
    "- Data must be manually stashed to SRAM prior to being re-used, and managed so as to minimize shared memory bank conflicts upon retrieval.\n",
    "- Computations must be partitioned and scheduled carefully, both across and within Streaming Multiprocessors (SMs), so as to promote instruction/thread-level parallelism and leverage special-purpose ALUs (e.g., tensor cores).\n",
    "\n",
    "Triton tries to abstact aways most of these challenges away. For example:\n",
    "\n",
    "- The only way to interact with DRAM is through `load/store` operations. One still has to be careful and cognizant of memory layouts. More on that later.\n",
    "- The result of `load` is managed automatically and will be placed into SRAM.\n",
    "- All high-level functionality is done in SRAM.\n",
    "- Scheduling is done automatically. SM resources (shared memory, registers, number of blocks to be processed) are limited, and it is very easy to make a mistake which will result in hardware underutilization.\n",
    "\n",
    "Fundamentally, CUDA C kernels operate per-thread basis, while Triton assumes per-block paralellism."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12098711",
   "metadata": {},
   "source": [
    "## Import and fix seeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b683c37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fabd8f4ab70>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "TORCH_SEED = 17\n",
    "TRITON_SEED = 13\n",
    "\n",
    "torch.manual_seed(TORCH_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdacd3c2",
   "metadata": {},
   "source": [
    "## Example 1: unary element-wise operations.\n",
    "\n",
    "Unary element-wise operations are among the easiest to parallelize and learn basic Triton from.\n",
    "Let's implement a kernel that fuses ReLU (Rectified Linear Unit) with dropout.\n",
    "\n",
    "Recall that\n",
    "\n",
    "\\begin{align}\n",
    "ReLU(x) & = \\max(x, 0), \\\\\n",
    "dropout(x, p) & = \\begin{cases}\n",
    " 0 \\text{ w. p. } p, \\\\\n",
    " \\frac{x}{1-p} \\text{ w. p. } 1 - p,\n",
    "\\end{cases}\n",
    "\\text{, so that } \\mathbb{E}[dropout(x, p)] = x.\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79b9ed6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit                                                                                                            \n",
    "def _triton_relu_dropout_kernel(in_ptr, out_ptr, numel, p, seed, BLOCK: tl.constexpr):\n",
    "    # Get block id.                                        \n",
    "    block_id = tl.program_id(axis=0)              \n",
    "    # Pointer offsets per element in a block.                                                                          \n",
    "    offset = block_id * BLOCK + tl.arange(0, BLOCK)        \n",
    "    # Load data but make sure no pointer access past numel.\n",
    "    # NOTE: in_ptr + offset is a block of pointers!\n",
    "    res = tl.load(in_ptr + offset, mask=offset < numel)                                                                \n",
    "    # Sample dropout probabilities.                        \n",
    "    probs = tl.rand(seed, offset)                                                                                      \n",
    "    # Apply relu and dropout.                              \n",
    "    res = tl.where(res > 0 and probs > p, res / (1. - p), 0.)                                                          \n",
    "    # Write to DRAM but make sure no pointer access past numel.                                                        \n",
    "    tl.store(out_ptr + offset, res, mask=offset < numel)                                                               \n",
    "\n",
    "# Public API\n",
    "def triton_relu_dropout(x, p=0.5, seed=TRITON_SEED, BLOCK=1024):\n",
    "    # ND array -> 1D array                                 \n",
    "    x_vector = x.reshape(-1)                                                                                           \n",
    "    out_vector = torch.empty_like(x_vector)                                                                            \n",
    "    grid = (triton.cdiv(x_vector.numel(), BLOCK),)                                                                     \n",
    "    _triton_relu_dropout_kernel[grid](x_vector, out_vector, x_vector.numel(), p, seed, BLOCK=BLOCK)\n",
    "                                                                                                                       \n",
    "    return out_vector.reshape(x.shape)\n",
    "\n",
    "# PyTorch eager counterpart\n",
    "def pytorch_relu_dropout(x, p=0.5):                        \n",
    "    import torch.nn.functional as F                                                                                    \n",
    "    return F.dropout(F.relu(x), p=p, inplace=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac200e4b",
   "metadata": {},
   "source": [
    "Some basic benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a72c16a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47.5 µs ± 292 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "119 µs ± 54.4 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "36.6 µs ± 66.4 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n",
      "55.3 µs ± 50.7 ns per loop (mean ± std. dev. of 7 runs, 10,000 loops each)\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(1000, 1000, device='cuda')  # float32 by default\n",
    "%timeit triton_relu_dropout(x); torch.cuda.synchronize()  # NEVER FORGET to sync devices!!!\n",
    "%timeit pytorch_relu_dropout(x); torch.cuda.synchronize()\n",
    "\n",
    "x = x.to(torch.half)\n",
    "%timeit triton_relu_dropout(x); torch.cuda.synchronize()\n",
    "%timeit pytorch_relu_dropout(x); torch.cuda.synchronize()\n",
    "\n",
    "# bfloat16 requires GPUs with compute capability >= 8.0\n",
    "# x = x.to(torch.bfloat16)\n",
    "# %timeit triton_relu_dropout(x); torch.cuda.synchronize()\n",
    "# %timeit pytorch_relu_dropout(x); torch.cuda.synchronize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36309557",
   "metadata": {},
   "source": [
    "##### Not bad! Now we know how to fuse several unary kernels into one and turn it from memory bound into compute bound!\n",
    "Alternatively, one could use `torch.jit` and the latest coolest feature of PyTorch2 - `torch.compile`.\n",
    "We will not discuss these much here, however."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f2d712",
   "metadata": {},
   "source": [
    "## Example 2: operations with reductions.\n",
    "\n",
    "Implementing efficient reductions in CUDA C is not at all trivial and requires some expert level skills.\n",
    "But first, some ...\n",
    "\n",
    "### Preliminaries.\n",
    "#### Addresses of elements in an ND array.\n",
    "Since Triton uses pointer arithmetic, we need to know how to offset pointers to right positions in an ND array.\n",
    "\n",
    "If $A$ is an array of dimension $d$, then the address of element $A[i_0, ..., i_{d-1}]$, id($A[i]$), is \n",
    "$\\mathbf{id(A) + i_0 \\cdot A.stride(0) + ... + i_{d-1} \\cdot A.stride(d-1)}.$ \n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70762c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = torch.arange(3 * 4 * 5)\n",
    "t3 = t.reshape(3, 4, 5)\n",
    "assert t3[2, 3, 1] == t[2 * t3.stride(0) + 3 * t3.stride(1) + 1 * t3.stride(2)]\n",
    "assert t3[1, 2, 3] == t[1 * t3.stride(0) + 2 * t3.stride(1) + 3 * t3.stride(2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cabcb9d",
   "metadata": {},
   "source": [
    "#### Memory contiguity.\n",
    "Moving data from/to DRAM is fast when the chunk of memory that is being read from/written to is contiguous through the so called DRAM bursts.\n",
    "\n",
    "SRAM, however, does not have such restrition and could be accessed in a non-contiguous manner.\n",
    "\n",
    "We should be cognizant of that because, even though Triton handles most cases of data movement efficiently, it does not handle them all. You will see examples of these down below.\n",
    "\n",
    "We say that a ND array $A$ is $\\mathbf{contiguous}$ if for any indices\n",
    "$i = (i_1, ..., i_d)$ and $j = (j_1, ..., j_d)$ such that $i < j$ lexicographically we have that\n",
    "\n",
    "\\begin{align}\n",
    "\\text{id}(A) &\\le \\text{id}(A[i]) < \\text{id}(A[j]), \\text{ and} \\\\\n",
    "\\text{id}(A[i]) & < \\text{id}(A) + A.\\text{size, if you prefer NumPy or } \\\\\n",
    "\\text{id}(A[i]) & < \\text{id}(A) + A.\\text{numel()/size(), if you prefer PyTorch}.\n",
    "\\end{align}\n",
    "\n",
    "Equivalently, using strides, $A$ is $\\mathbf{contiguous}$ if\n",
    "\\begin{align}\n",
    "A.stride(-1) &= 1, \\\\\n",
    "A.stride(-i-1) &= A.shape[-i] \\cdot A.stride(-i), \\text{ for } i \\in \\{1, ..., A.dim()\\}.\n",
    "\\end{align}\n",
    "$\\mathbf{NOTE:}$ the strides are non-negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8256017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 1)\n",
      "(1, 3)\n"
     ]
    }
   ],
   "source": [
    "t = torch.rand(2, 3, 4, 5)\n",
    "assert t.is_contiguous()\n",
    "\n",
    "def check_contiguous(t):\n",
    "    isc = (t.stride(-1) == 1)\n",
    "    for i in range(1, t.dim()):\n",
    "        isc = (t.stride(-i-1) == t.shape[-i] * t.stride(-i)) and isc\n",
    "    return isc\n",
    "\n",
    "assert check_contiguous(t) == True\n",
    "assert check_contiguous(t.transpose(-1, -2)) == False\n",
    "assert check_contiguous(t.transpose(0, -1)) == False\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "print(x.stride())  # x is row-major\n",
    "y = x.mT.contiguous().mT\n",
    "print(y.stride())  # y is column-major"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afe8d04",
   "metadata": {},
   "source": [
    "$A$ is $\\mathbf{Triton-contiguous}$ if just id($A) \\le $ id($A[i]) < $ id($A$) + $A$.size hold for all $i < A$.shape.\n",
    "\n",
    "As long as the DRAM block that we read from/write to is $\\mathbf{Triton-contiguous}$, all `tl.load/tl.store` operations with it are going to be efficient as well!\n",
    "\n",
    "In most practical cases (see below) we can also be fine as long as there is a single dimension with stride 1, provided that it is large enough compared to other dims. Triton compiler is smart and will organize contiguous DRAM-SRAM movements if strides permit that! For example, when working with square 2D block, it does not matter if they are row-major or column-major."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be2fd4c",
   "metadata": {},
   "source": [
    "### Numerically stable softmax\n",
    "\n",
    "Recall that a softmax is a map\n",
    "\\begin{align}\n",
    "\\text{softmax}: x=(x_1, ..., x_n) \\mapsto \n",
    "    \\bigg( \\frac{\\exp(x_1)}{\\exp(\\sum_{i=1}^n x_i)}, ...,\n",
    "           \\frac{\\exp(x_n)}{\\exp(\\sum_{i=1}^n x_n}  \\bigg).\n",
    "\\end{align}\n",
    "\n",
    "We can see that softmax($x) = $ softmax($x + \\alpha$) for any scalar $\\alpha$. To utilize mantissa better, $\\alpha$ is chosen to be $\\alpha = -\\max(x_1, ..., x_n)$.\n",
    "\n",
    "This is the stable kernel we want to implement, but we will further generalize it to ND arrays with the `dim` argument support similar to what PyTorch has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eef9c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit                                                                                                                                                                                   \n",
    "def _triton_softmax_kernel(                                                                    \n",
    "    in_ptr, out_ptr,                                                                           \n",
    "    row_stride,                                                                                \n",
    "    dim_size,                                                                                  \n",
    "    BLOCK: tl.constexpr):                                                                      \n",
    "    row_idx = tl.program_id(axis=0)                                                            \n",
    "    in_row_ptr = in_ptr + row_idx * row_stride                                                 \n",
    "    out_row_ptr = out_ptr + row_idx * row_stride                                               \n",
    "                                                                                               \n",
    "    ##################################                                                                                                                                                        \n",
    "    # Stage 1. Find max in each row. #                                                         \n",
    "    ##################################                                                         \n",
    "    col_offsets = tl.arange(0, BLOCK)                                                          \n",
    "    col_tile = tl.load(in_row_ptr + col_offsets, mask=col_offsets < dim_size, other=-float('inf'))                                                                                            \n",
    "    max_col_val = tl.max(col_tile, axis=0)                                                     \n",
    "    for _ in range(BLOCK, dim_size, BLOCK):                                                    \n",
    "        col_offsets += BLOCK                                                                   \n",
    "        col_tile = tl.load(in_row_ptr + col_offsets, mask=col_offsets < dim_size, other=-float('inf'))                                                                                        \n",
    "        curr_col_max = tl.max(col_tile, axis=0)                                                \n",
    "        max_col_val = tl.where(max_col_val > curr_col_max, max_col_val, curr_col_max)                                                                                                         \n",
    "                                                                                                                                                                                              \n",
    "    ##############################                                                                                                                                                            \n",
    "    # Stage 2. Find denominator. #                                                             \n",
    "    ##############################                                                             \n",
    "    num = tl.exp(col_tile - max_col_val)                                                       \n",
    "    denom = tl.sum(num, axis=0)                                                                \n",
    "    for _ in range(BLOCK, dim_size, BLOCK):                                                                                                                                                   \n",
    "        col_offsets -= BLOCK                                                                                                                                                                  \n",
    "        col_tile = tl.load(in_row_ptr + col_offsets, mask=col_offsets < dim_size, other=-float('inf'))                                                                                        \n",
    "        num = tl.exp(col_tile - max_col_val)                                                   \n",
    "        denom += tl.sum(num, axis=0)                                                                                                                                                          \n",
    "                                                                                               \n",
    "    #############################                                                                                                                                                             \n",
    "    # Stage 3. Populate output. #                                                              \n",
    "    #############################                                                              \n",
    "    tl.store(out_row_ptr + col_offsets, num / denom, mask=col_offsets < dim_size)                                                                                                             \n",
    "    for _ in range(BLOCK, dim_size, BLOCK):                                                    \n",
    "        col_offsets += BLOCK                                                                                                                                                                  \n",
    "        col_tile = tl.load(in_row_ptr + col_offsets, mask=col_offsets < dim_size)                                                                                                             \n",
    "        num = tl.exp(col_tile - max_col_val)                                                                                                                                                  \n",
    "        tl.store(out_row_ptr + col_offsets, num / denom, mask=col_offsets < dim_size)                                                                                                         \n",
    "\n",
    "# Public API\n",
    "def triton_softmax(x, dim=-1, BLOCK=None):                                                     \n",
    "    assert -x.dim() <= dim < x.dim()                                                           \n",
    "    dim = dim + x.dim() if dim < 0 else dim                                                    \n",
    "    dim_size = x.shape[dim]                                                                    \n",
    "                                                                                               \n",
    "    # Make sure scans/reductions are done over contiguous dim.                                         \n",
    "    xt = x.transpose(dim, -1)                                                                  \n",
    "    if xt.stride(-1) != 1:                                                                     \n",
    "        xt = xt.contiguous()                                                                                                                                                                  \n",
    "    xt_shape = xt.shape                                                                        \n",
    "    # Squash first xt.dim() - 1 dims.                                                          \n",
    "    xt_2d = xt.reshape(-1, dim_size)                                                           \n",
    "                                                                                               \n",
    "    # Allocate output                                                                          \n",
    "    res_2d = torch.empty_like(xt_2d)                                                           \n",
    "     \n",
    "    # NOTE: we can use blocks larger than CUDA blocks.\n",
    "    if BLOCK is None:                                                                          \n",
    "        BLOCK = triton.next_power_of_2(xt_2d.size(-1))\n",
    "    \n",
    "    # In modern GPUs, warp is a group of 32 threads\n",
    "    # that execute the same instruction set in a SIMD-like manner.\n",
    "    # The larger the block, the more threads we put to work (i.e. larger CUDA block).\n",
    "    num_warps = 4                                                                                                                                                                             \n",
    "    if BLOCK >= 2048:                                                                                                                                                                         \n",
    "        num_warps = 8                                                                                                                                                                         \n",
    "    if BLOCK >= 4096:                                                                          \n",
    "        num_warps = 16                                                                         \n",
    "    # Launch kernel, parallelization over rows.                                                                                                                                                                           \n",
    "    grid = (xt_2d.size(0),)                                                                    \n",
    "    _triton_softmax_kernel[grid](xt_2d, res_2d, xt_2d.stride(0), dim_size, BLOCK=BLOCK, num_warps=num_warps)                                                                                  \n",
    "                                                                                                                                                                                              \n",
    "    return res_2d.reshape(xt_shape).transpose(dim, -1)                                         \n",
    "                                                                                               \n",
    "def pytorch_softmax(x, dim=-1, **kwargs):                                                                                                                                                     \n",
    "    return torch.nn.functional.softmax(x, dim=dim)                                             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7216090a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3388e-09, device='cuda:0')\n",
      "tensor(1.3388e-09, device='cuda:0')\n",
      "tensor(1.7462e-10, device='cuda:0')\n",
      "tensor(2.3283e-10, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3000, 3000, device='cuda')\n",
    "for dim in (0, 1):\n",
    "    x_pt = pytorch_softmax(x, dim=dim)\n",
    "    for block in (None, 1024):\n",
    "        x_tri = triton_softmax(x, dim=dim, BLOCK=block)\n",
    "        print((x_tri - x_pt).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88176506",
   "metadata": {},
   "source": [
    "Some benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "191a3737",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-------------------- softmax 100 x 100 ---------------------]\n",
      "                         |  triton_softmax  |  pytorch_softmax\n",
      "1 threads: ---------------------------------------------------\n",
      "      dim=0, block=512   |        30        |                 \n",
      "      dim=0, block=1024  |        33        |                 \n",
      "      dim=0, block=None  |        31        |        32.5     \n",
      "      dim=1, block=512   |        20        |                 \n",
      "      dim=1, block=1024  |        19        |                 \n",
      "      dim=1, block=None  |        20        |         5.2     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[-------------------- softmax 500 x 500 ---------------------]\n",
      "                         |  triton_softmax  |  pytorch_softmax\n",
      "1 threads: ---------------------------------------------------\n",
      "      dim=0, block=512   |        30        |                 \n",
      "      dim=0, block=1024  |        30        |                 \n",
      "      dim=0, block=None  |        33        |        245      \n",
      "      dim=1, block=512   |        20        |                 \n",
      "      dim=1, block=1024  |        20        |                 \n",
      "      dim=1, block=None  |        20        |          5      \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[------------------- softmax 1000 x 1000 --------------------]\n",
      "                         |  triton_softmax  |  pytorch_softmax\n",
      "1 threads: ---------------------------------------------------\n",
      "      dim=0, block=512   |       77.9       |                 \n",
      "      dim=0, block=1024  |       78.9       |                 \n",
      "      dim=0, block=None  |       78.9       |       990.7     \n",
      "      dim=1, block=512   |       29.0       |                 \n",
      "      dim=1, block=1024  |       29.8       |                 \n",
      "      dim=1, block=None  |       29.8       |        29.6     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[------------------- softmax 2000 x 2000 --------------------]\n",
      "                         |  triton_softmax  |  pytorch_softmax\n",
      "1 threads: ---------------------------------------------------\n",
      "      dim=0, block=512   |       362        |                 \n",
      "      dim=0, block=1024  |       358        |                 \n",
      "      dim=0, block=None  |       330        |        2082     \n",
      "      dim=1, block=512   |       144        |                 \n",
      "      dim=1, block=1024  |       140        |                 \n",
      "      dim=1, block=None  |       112        |         124     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[------------------- softmax 3000 x 3000 --------------------]\n",
      "                         |  triton_softmax  |  pytorch_softmax\n",
      "1 threads: ---------------------------------------------------\n",
      "      dim=0, block=512   |       906        |                 \n",
      "      dim=0, block=1024  |       896        |                 \n",
      "      dim=0, block=None  |       787        |        3252     \n",
      "      dim=1, block=512   |       368        |                 \n",
      "      dim=1, block=1024  |       359        |                 \n",
      "      dim=1, block=None  |       249        |         325     \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer, Compare\n",
    "ns = (100, 500, 1000, 2000, 3000)\n",
    "bench_results = []\n",
    "for n in ns:\n",
    "    x = torch.rand(n, n, device='cuda')\n",
    "    label = f\"softmax {n} x {n}\"\n",
    "    for dim in (0, 1):\n",
    "        for f in (triton_softmax, pytorch_softmax):\n",
    "            if f is triton_softmax:\n",
    "                blocks = (512, 1024, None)\n",
    "            else:\n",
    "                blocks = (None,)\n",
    "            for block in blocks:\n",
    "                sub_label = f\"dim={dim}, block={block}\"\n",
    "                smpt = f\"{f.__name__}(x, dim={dim}, BLOCK={block})\"\n",
    "                timer = Timer(smpt,\n",
    "                                globals=globals(),\n",
    "                                label=label,\n",
    "                                description=f\"{f.__name__}\",\n",
    "                                sub_label=sub_label)\n",
    "                bench_results.append(timer.blocked_autorange())\n",
    "\n",
    "compare = Compare(bench_results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab89aeb",
   "metadata": {},
   "source": [
    "## Example 3: matrix multiplication\n",
    "\n",
    "Given matrices $A, B$ of shape $(..., m, k), (..., k, n)$, respectively, the goal is to produce their matrix product of shape $(..., m, n)$.\n",
    "\n",
    "We split dimensions $m, n, k$ into tiles of size BLOCK_M, BLOCK_N, BLOCK_K and compute a block-wise matrix multiplication\n",
    "\n",
    "\\begin{align}\n",
    "C[..., b_m(i), b_n(j)] &= \\sum_k A[..., b_m(i), b_k(k)] @ B[..., b_k(k), b_n(j)], \\text{ where} \\\\\n",
    "b_m(i) &= i \\cdot \\text{BLOCK_M}:i \\cdot \\text{BLOCK_M} + \\text{BLOCK_M}, \\\\\n",
    "b_n(j) &= j \\cdot \\text{BLOCK_N}:j \\cdot \\text{BLOCK_N} + \\text{BLOCK_N}, \\\\\n",
    "b_k(k) &= k \\cdot \\text{BLOCK_K}:k \\cdot \\text{BLOCK_K} + \\text{BLOCK_K}, \\\\\n",
    "i & \\in \\text{range}\\bigg(1, \\bigg\\lceil \\frac{m}{\\text{BLOCK_M}} \\bigg\\rceil\\bigg), \\\\\n",
    "j & \\in \\text{range}\\bigg(1, \\bigg\\lceil \\frac{n}{\\text{BLOCK_N}} \\bigg\\rceil\\bigg), \\\\\n",
    "k & \\in \\text{range}\\bigg(1, \\bigg\\lceil \\frac{k}{\\text{BLOCK_K}} \\bigg\\rceil\\bigg). \\\\\n",
    "\\end{align}\n",
    "\n",
    "We parallelize over a 3D grid of size $(n, m, ...)$. Left-most coordinates advance first, so the blocks are row-major ordered! There is a problem with that, however.\n",
    "\n",
    "For example, in the following matmul where each matrix is 9 blocks by 9 blocks, we can see that if we compute the output in row-major ordering, we need to load 90 blocks into SRAM to compute the first 9 output blocks, but if we do it in grouped ordering, we only need to load 54 blocks.\n",
    "\n",
    "<img src=\"https://triton-lang.org/main/_images/grouped_vs_row_major_ordering.png\" width=\"900\" height=\"600\">\n",
    "Source: https://triton-lang.org/main/_images/grouped_vs_row_major_ordering.png\n",
    "\n",
    "Luckily, Triton has a function for such orderings, `tl.swizzle2d`. We will use it to improve cache locality in the code below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3b9da48",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def _triton_matmul_kernel(\n",
    "    x_ptr, y_ptr, out_ptr,\n",
    "    x_b_stride, x_m_stride, x_k_stride,\n",
    "    y_b_stride, y_k_stride, y_n_stride,\n",
    "    out_b_stride, out_m_stride, out_n_stride,\n",
    "    m, n, k,\n",
    "    BLOCK_M: tl.constexpr,\n",
    "    BLOCK_N: tl.constexpr,\n",
    "    BLOCK_K: tl.constexpr):\n",
    "    batch_idx = tl.program_id(axis=2)\n",
    "    block_row_idx = tl.program_id(axis=1)\n",
    "    block_col_idx = tl.program_id(axis=0)\n",
    "    n_block_rows = tl.num_programs(axis=1)\n",
    "    n_block_cols = tl.num_programs(axis=0)\n",
    "    \n",
    "    # Row-major order to group order for better L2 re-use.\n",
    "    block_row_idx, block_col_idx = tl.swizzle2d(\n",
    "        block_row_idx, block_col_idx,\n",
    "        n_block_rows, n_block_cols,\n",
    "        9\n",
    "    )\n",
    "\n",
    "    block_m_offsets = tl.arange(0, BLOCK_M)\n",
    "    block_n_offsets = tl.arange(0, BLOCK_N)\n",
    "    block_k_offsets = tl.arange(0, BLOCK_K)\n",
    "\n",
    "    # x_row = &x[b, b_m(block_row_idx), b_k(0)]\n",
    "    x_row = (x_ptr + batch_idx * x_b_stride\n",
    "           + block_row_idx * BLOCK_M * x_m_stride\n",
    "           + block_m_offsets[:, None] * x_m_stride\n",
    "           + block_k_offsets[None, :] * x_k_stride)\n",
    "\n",
    "    # y_col = &y[b, b_k(0), b_n(block_col_idx)]\n",
    "    y_col = (y_ptr + batch_idx * y_b_stride\n",
    "           + block_col_idx * BLOCK_N * y_n_stride\n",
    "           + block_k_offsets[:, None] * y_k_stride\n",
    "           + block_n_offsets[None, :] * y_n_stride)\n",
    "\n",
    "    acc = tl.zeros((BLOCK_M, BLOCK_N), dtype=tl.float32)\n",
    "    for block_offset in range(0, k, BLOCK_K):\n",
    "        mask_k = (block_offset + block_k_offsets) < k\n",
    "        x_block = tl.load(x_row, mask=mask_k[None, :], other=0.0)\n",
    "        y_block = tl.load(y_col, mask=mask_k[:, None], other=0.0)\n",
    "        acc += tl.dot(x_block, y_block)\n",
    "        # advance pointers\n",
    "        x_row += BLOCK_K * x_k_stride\n",
    "        y_col += BLOCK_K * y_k_stride\n",
    "        \n",
    "    # These are not used in the loop above.\n",
    "    # Them re-materialization tells compiler\n",
    "    # that their memory could be re-used.\n",
    "    # Registers are super fast! The more we have them, the better!\n",
    "    # Also, register spills reduce occupancy!\n",
    "    block_m_offsets = tl.arange(0, BLOCK_M)\n",
    "    block_n_offsets = tl.arange(0, BLOCK_N)\n",
    "    \n",
    "    \n",
    "    mask_m = (block_row_idx * BLOCK_M + block_m_offsets) < m\n",
    "    mask_n = (block_col_idx * BLOCK_N + block_n_offsets) < n\n",
    "\n",
    "    out = (out_ptr + batch_idx * out_b_stride\n",
    "         + block_row_idx * BLOCK_M * out_m_stride\n",
    "         + block_col_idx * BLOCK_N * out_n_stride\n",
    "         + block_m_offsets[:, None] * out_m_stride\n",
    "         + block_n_offsets[None, :] * out_n_stride)\n",
    "    tl.store(out, acc.to(out_ptr.dtype.element_ty), mask=(mask_m[:, None] & mask_n[None, :]))\n",
    "\n",
    "def triton_matmul(x, y, BLOCK_M=None, BLOCK_N=None, BLOCK_K=None):\n",
    "    assert x.dim() >= 2 and y.dim() >= 2\n",
    "    assert x.dtype == y.dtype and x.device == y.device\n",
    "    m, k = x.shape[-2:]\n",
    "    k1, n = y.shape[-2:]\n",
    "    assert k == k1\n",
    "    x_shape = list(x.shape)\n",
    "    y_shape = list(y.shape)\n",
    "    x_shape[-1] = 1\n",
    "    y_shape[-2] = 1\n",
    "    out_shape = torch.broadcast_shapes(x_shape, y_shape)\n",
    "    out = torch.empty(*out_shape, dtype=x.dtype, device=x.device)\n",
    "\n",
    "    # make sure that x and y are \"Triton-contiguous\"\n",
    "    def make_triton_contiguous(t):\n",
    "        # return t if it is already row-/col-major\n",
    "        if t.is_contiguous() or t.mT.is_contiguous():\n",
    "            return t\n",
    "        else:\n",
    "            return t.contiguous()\n",
    "\n",
    "    x = make_triton_contiguous(x)\n",
    "    y = make_triton_contiguous(y)\n",
    "\n",
    "    def to_3d(t):\n",
    "        return t.reshape(-1, *t.shape[-2:])\n",
    "\n",
    "    x_3d = to_3d(x)\n",
    "    y_3d = to_3d(y)\n",
    "    out_3d = to_3d(out)  # will always be a view!\n",
    "\n",
    "    # Launch kernel\n",
    "    BLOCK_M, BLOCK_N, BLOCK_K = map(lambda t: 64 if t is None else t, (BLOCK_M, BLOCK_N, BLOCK_K))\n",
    "    b = out_3d.size(0)\n",
    "    grid = (triton.cdiv(n, BLOCK_N), triton.cdiv(m, BLOCK_M), b)\n",
    "    _triton_matmul_kernel[grid](x_3d, y_3d, out_3d,\n",
    "                                *x_3d.stride(), *y_3d.stride(), *out_3d.stride(),\n",
    "                                m, n, k,\n",
    "                                BLOCK_M=BLOCK_M,\n",
    "                                BLOCK_N=BLOCK_N,\n",
    "                                BLOCK_K=BLOCK_K,\n",
    "                                num_stages=1)\n",
    "\n",
    "    return out_3d.reshape(out_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fb76a41d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0002, device='cuda:0', dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "BLOCK=64\n",
    "n = 4000\n",
    "x = torch.rand(4, n, n, device='cuda', dtype=torch.half) / BLOCK\n",
    "y = triton_matmul(x, x.mT)\n",
    "z = x @ x.mT\n",
    "print((y - z).abs().max())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6884ac3",
   "metadata": {},
   "source": [
    "Some benchmarks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71e22fbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[------------------------------------------ mm 100 x 100 -----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |       40.4      |                  |                      \n",
      "      dtype=torch.float32, block=32    |       40.8      |                  |                      \n",
      "      dtype=torch.float32, block=None  |       42.9      |       9.4        |          39.7        \n",
      "      dtype=torch.float16, block=32    |       40.1      |                  |                      \n",
      "      dtype=torch.float16, block=None  |       40.2      |       9.8        |          40.0        \n",
      "      dtype=torch.float16, block=128   |       46.6      |                  |                      \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[------------------------------------------ mm 500 x 500 -----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |       447       |                  |                      \n",
      "      dtype=torch.float32, block=32    |       374       |                  |                      \n",
      "      dtype=torch.float32, block=None  |       223       |        60        |          209         \n",
      "      dtype=torch.float16, block=32    |        48       |                  |                      \n",
      "      dtype=torch.float16, block=None  |        45       |        29        |           81         \n",
      "      dtype=torch.float16, block=128   |       388       |                  |                      \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[----------------------------------------- mm 1000 x 1000 ----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |       3335      |                  |                      \n",
      "      dtype=torch.float32, block=32    |       2875      |                  |                      \n",
      "      dtype=torch.float32, block=None  |       1503      |       376        |          1540        \n",
      "      dtype=torch.float16, block=32    |        350      |                  |                      \n",
      "      dtype=torch.float16, block=None  |        240      |       100        |           416        \n",
      "      dtype=torch.float16, block=128   |       2636      |                  |                      \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[----------------------------------------- mm 2000 x 2000 ----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |      25290      |                  |                      \n",
      "      dtype=torch.float32, block=32    |      21870      |                  |                      \n",
      "      dtype=torch.float32, block=None  |      12000      |       2836       |         11700        \n",
      "      dtype=torch.float16, block=32    |       2202      |                  |                      \n",
      "      dtype=torch.float16, block=None  |       1400      |        670       |          1700        \n",
      "      dtype=torch.float16, block=128   |       1830      |                  |                      \n",
      "\n",
      "Times are in microseconds (us).\n",
      "\n",
      "[----------------------------------------- mm 3000 x 3000 ----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |       88.4      |                  |                      \n",
      "      dtype=torch.float32, block=32    |       71.8      |                  |                      \n",
      "      dtype=torch.float32, block=None  |       36.1      |       9.1        |         37.43        \n",
      "      dtype=torch.float16, block=32    |        8.8      |                  |                      \n",
      "      dtype=torch.float16, block=None  |        5.6      |       2.2        |          6.64        \n",
      "      dtype=torch.float16, block=128   |       64.2      |                  |                      \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "[----------------------------------------- mm 4000 x 4000 ----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |       206       |                  |                      \n",
      "      dtype=torch.float32, block=32    |       168       |                  |                      \n",
      "      dtype=torch.float32, block=None  |        87       |       41.2       |           88         \n",
      "      dtype=torch.float16, block=32    |        16       |                  |                      \n",
      "      dtype=torch.float16, block=None  |        11       |        5.2       |           13         \n",
      "      dtype=torch.float16, block=128   |        14       |                  |                      \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n",
      "[----------------------------------------- mm 5000 x 5000 ----------------------------------------]\n",
      "                                       |  triton_matmul  |  pytorch_matmul  |  triton_native_matmul\n",
      "1 threads: ----------------------------------------------------------------------------------------\n",
      "      dtype=torch.float32, block=16    |      407.9      |                  |                      \n",
      "      dtype=torch.float32, block=32    |      334.0      |                  |                      \n",
      "      dtype=torch.float32, block=None  |      170.7      |       120        |          175         \n",
      "      dtype=torch.float16, block=32    |       42.0      |                  |                      \n",
      "      dtype=torch.float16, block=None  |       26.5      |        10        |           32         \n",
      "      dtype=torch.float16, block=128   |      294.0      |                  |                      \n",
      "\n",
      "Times are in milliseconds (ms).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.benchmark import Timer, Compare\n",
    "from triton.ops import matmul\n",
    "\n",
    "def pytorch_matmul(x, y, **kwargs):\n",
    "    return x @ y\n",
    "\n",
    "def triton_native_matmul(x, y, **kwargs):\n",
    "    return matmul(x, y)\n",
    "\n",
    "ns = (100, 500, 1000, 2000, 3000, 4000, 5000)\n",
    "bench_results = []\n",
    "for dtype in (torch.float, torch.half):\n",
    "    for n in ns:\n",
    "        x = torch.rand(n, n, device='cuda', dtype=dtype)\n",
    "        label = f\"mm {n} x {n}\"\n",
    "        for f in (triton_matmul, pytorch_matmul, triton_native_matmul):\n",
    "            if dtype is torch.float:\n",
    "                blocks = (16, 32, None)\n",
    "            else:\n",
    "                blocks = (32, None, 128)\n",
    "                \n",
    "            if f is triton_native_matmul:\n",
    "                # Warm-up!\n",
    "                triton_native_matmul(x, x.mT)\n",
    "            \n",
    "            if f is not triton_matmul:\n",
    "                blocks = (None,)\n",
    "                kwarg_str = str()\n",
    "            else:\n",
    "                kwarg_str = \"BLOCK_M=block, BLOCK_N=block, BLOCK_K=block\"\n",
    "                \n",
    "            for block in blocks:\n",
    "                sub_label = f\"dtype={dtype}, block={block}\"\n",
    "                smpt = f\"{f.__name__}(x, x.mT, \" + kwarg_str + \")\" \n",
    "                timer = Timer(smpt,\n",
    "                                globals=globals(),\n",
    "                                label=label,\n",
    "                                description=f\"{f.__name__}\",\n",
    "                                sub_label=sub_label)\n",
    "                bench_results.append(timer.blocked_autorange())\n",
    "\n",
    "compare = Compare(bench_results)\n",
    "compare.trim_significant_figures()\n",
    "compare.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3aea6e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "### Triton\n",
    "- [Triton library documentation](https://triton-lang.org/main/index.html)\n",
    "- [Triton GitHub page](https://github.com/openai/triton)\n",
    "- [Tillet, Philippe G. 2020. Blocked Algorithms for Neural Networks: Design and Implementation on\n",
    "GPUs. Doctoral dissertation, Harvard University Graduate School of Arts and Sciences](https://dash.harvard.edu/bitstream/handle/1/37368966/ptillet-dissertation-final.pdf)\n",
    "\n",
    "### GPU programming with CUDA C\n",
    "- [Programming Massively Parallel Processors. A Hands-on Approach by Wen-mei Hwu, David Kirk, Izzat El Hajj](https://www.elsevier.com/books/programming-massively-parallel-processors/hwu/978-0-323-91231-0)\n",
    "- [Professional CUDA C Programming by John Cheng, Max Grossman, Ty McKercher](https://www.wiley.com/en-fr/Professional+CUDA+C+Programming-p-9781118739327)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
